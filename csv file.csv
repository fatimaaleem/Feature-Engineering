Cell ID,Code
fZ25BuA6VS48,
483a9932,"print(""Data visualization is crucial in data analysis and communication because it allows us to quickly understand patterns, trends, and outliers that might be hidden within raw data. Visual representations make complex information more accessible and help in conveying insights effectively to various audiences."")
print(""\nMatplotlib is a foundational plotting library in Python. Its strength lies in its flexibility and fine-grained control over every element of a plot, making it suitable for creating highly customized visualizations. However, this flexibility often comes at the cost of verbosity, especially when creating complex plots."")
print(""\nSeaborn is a library built on top of Matplotlib. It simplifies the creation of aesthetically pleasing and statistically informative graphics. Its strength is its ease of use for common statistical plots, providing built-in themes and color palettes that produce visually appealing results with less code compared to Matplotlib."")
print(""\nPlotly is a library for creating interactive and web-based visualizations. Its key strengths are its interactivity, allowing users to hover, zoom, and pan, and its ability to generate standalone HTML files that can be easily shared. While powerful, Plotly might have a slightly steeper learning curve for some users compared to static libraries like Matplotlib and Seaborn."")"
eaf87f88,"import matplotlib.pyplot as plt
import numpy as np

# Step 1 & 2: Explain Figure, Axes, and plotting functions
print(""Matplotlib's core building blocks are the Figure and Axes."")
print(""A Figure is the overall window or page on which everything is drawn. Think of it as the canvas."")
print(""An Axes is the area within the Figure where the data is plotted. A Figure can contain multiple Axes."")
print(""Plotting functions like plot(), scatter(), bar(), and hist() are typically methods of the Axes object. They draw data onto that specific Axes."")

# Step 3: Simple line plot with customization
print(""\n--- Line Plot Example ---"")
fig1, ax1 = plt.subplots() # Create a Figure and an Axes.
x = np.linspace(0, 10, 100)
y = np.sin(x)
ax1.plot(x, y, label='sin(x)', color='blue', linestyle='--') # Plot data on the Axes
ax1.set_title('Simple Line Plot') # Set title
ax1.set_xlabel('x-axis') # Set x-label
ax1.set_ylabel('y-axis') # Set y-label
ax1.legend() # Add a legend
plt.show() # Display the plot

# Step 4: Simple scatter plot with customization
print(""\n--- Scatter Plot Example ---"")
fig2, ax2 = plt.subplots()
x_scatter = np.random.rand(50)
y_scatter = np.random.rand(50)
ax2.scatter(x_scatter, y_scatter, marker='o', color='red', alpha=0.7) # Customize marker and color
ax2.set_title('Simple Scatter Plot')
ax2.set_xlabel('Random X')
ax2.set_ylabel('Random Y')
plt.show()

# Step 5: Simple bar plot with customization
print(""\n--- Bar Plot Example ---"")
fig3, ax3 = plt.subplots()
categories = ['A', 'B', 'C', 'D']
values = [25, 50, 30, 45]
bars = ax3.bar(categories, values, color=['purple', 'orange', 'green', 'brown']) # Create bar plot
ax3.set_title('Simple Bar Plot')
ax3.set_xlabel('Category')
ax3.set_ylabel('Value')
ax3.set_xticks(np.arange(len(categories))) # Set x-axis ticks
ax3.set_xticklabels(categories) # Set x-axis labels
plt.show()

# Step 6: Simple histogram with customization
print(""\n--- Histogram Example ---"")
fig4, ax4 = plt.subplots()
data_hist = np.random.randn(200) # Generate some random data
ax4.hist(data_hist, bins=20, color='skyblue', edgecolor='black', alpha=0.7) # Set bins and edge color
ax4.set_title('Simple Histogram')
ax4.set_xlabel('Value')
ax4.set_ylabel('Frequency')
plt.show()"
5c1c94da,"import seaborn as sns
import matplotlib.pyplot as plt # Import matplotlib for displaying plots

# 1. Explain how Seaborn extends Matplotlib and its focus on statistical plotting.
print(""Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics. While Matplotlib provides the fundamental building blocks (Figures, Axes), Seaborn offers refined aesthetics and functions specifically designed for common statistical plotting tasks, making it easier to create complex visualizations like heatmaps, time series plots, and multi-variate distributions with less code."")
print(""Seaborn is particularly good at handling pandas DataFrames and automatically maps data columns to visual attributes, simplifying the process of creating informative plots from structured data."")

# 2. Discuss the key categories of plots Seaborn provides.
print(""\nSeaborn organizes its plotting functions into several categories:"")
print(""- **Distribution plots:** Show the distribution of a single variable or the relationship between two variables and their distributions (e.g., histplot, kdeplot, displot, rugplot, jointplot)."")
print(""- **Relationship plots:** Show the relationship between two or more variables (e.g., scatterplot, lineplot, relplot, regplot)."")
print(""- **Categorical plots:** Show the relationship between a numerical variable and one or more categorical variables (e.g., boxplot, violinplot, stripplot, swarmplot, barplot, countplot, pointplot, catplot)."")
print(""- **Regression plots:** Plot data and a linear regression model fit (e.g., regplot, lmplot)."")
print(""- **Matrix plots:** Plot data as a matrix (e.g., heatmap, clustermap)."")
print(""- **Multi-plot grids:** Draw plots onto a FacetGrid or PairGrid (e.g., FacetGrid, PairGrid)."")

# 3. Import the seaborn library (already done above)
# import seaborn as sns - This was done at the beginning of the code block.
"
00c07882,"# 4. Load one of Seaborn's built-in datasets
tips = sns.load_dataset('tips')
print(""\n--- First 5 rows of the 'tips' dataset ---"")
display(tips.head())

# 5. Create a distribution plot (e.g., histplot)
print(""\n--- Distribution Plot (Histogram of Total Bill) ---"")
sns.histplot(data=tips, x='total_bill', kde=True)
plt.title('Distribution of Total Bill')
plt.xlabel('Total Bill ($)')
plt.ylabel('Frequency')
plt.show()

# 6. Create a scatter plot with a regression line (regplot)
print(""\n--- Scatter Plot with Regression Line (Total Bill vs. Tip) ---"")
sns.regplot(data=tips, x='total_bill', y='tip')
plt.title('Total Bill vs. Tip with Regression Line')
plt.xlabel('Total Bill ($)')
plt.ylabel('Tip ($)')
plt.show()

# 7. Create a box plot (boxplot)
print(""\n--- Box Plot (Total Bill by Day) ---"")
sns.boxplot(data=tips, x='day', y='total_bill')
plt.title('Total Bill by Day')
plt.xlabel('Day of the Week')
plt.ylabel('Total Bill ($)')
plt.show()

# 8. Create a violin plot (violinplot)
print(""\n--- Violin Plot (Tip by Time) ---"")
sns.violinplot(data=tips, x='time', y='tip')
plt.title('Tip by Time of Day')
plt.xlabel('Time of Day')
plt.ylabel('Tip ($)')
plt.show()
"
470c2dcd,"# 1. Print an explanation of Plotly's strengths and structure.
print(""Plotly is a powerful interactive graphing library that excels in creating web-based visualizations. Its key strength lies in its built-in interactivity, allowing users to zoom, pan, hover, and select data points directly within the plot without needing additional code for these interactions."")
print(""\nPlotly's focus on web-based visualizations means that plots can be easily exported as standalone HTML files or integrated into web applications and dashboards, making them highly shareable and accessible."")
print(""\nPlotly's structure is based on JSON-like figure dictionaries or using the `graph_objects` module, which represents figures and their components as Python objects. The high-level `plotly.express` module simplifies creating complex plots with minimal code."")
print(""\nInteractivity in Plotly is primarily handled by JavaScript libraries (like Plotly.js) running in the web browser. When you create a plot in Python, Plotly generates a JSON representation of the figure, which is then interpreted by the JavaScript library to render the interactive plot in a web environment."")"
eba6c3ab,"# 2. Import the plotly.express module.
import plotly.express as px

# 3. Create an interactive scatter plot using plotly.express.
# Use the tips DataFrame, mapping 'total_bill' to x, 'tip' to y, color by 'sex', and include tooltips.
fig_scatter = px.scatter(tips,
                         x='total_bill',
                         y='tip',
                         color='sex', # Add color mapping by 'sex'
                         hover_data=['size', 'day', 'time', 'smoker'], # Add tooltips for relevant columns
                         title='Total Bill vs. Tip by Sex') # Add a title

# Display the plot.
fig_scatter.show()
"
587ed97f,"# 4. Create an interactive bar chart using plotly.express.
# Use the tips DataFrame, grouping by 'day' and showing the average 'total_bill'.
# Plotly Express automatically handles aggregation for bar charts if y is numerical and x is categorical.
fig_bar = px.bar(tips,
                 x='day',
                 y='total_bill',
                 title='Average Total Bill by Day', # Add a title
                 labels={'total_bill': 'Average Total Bill ($)', 'day': 'Day of the Week'}) # Add axis labels

# Display the plot.
fig_bar.show()
"
24b0fe28,"# 5. Create an interactive line plot using plotly.express.
# For simplicity, we'll sort by day and plot a cumulative sum to show a trend-like visualization.
# First, ensure the 'day' column has a defined order for plotting.
day_order = ['Thur', 'Fri', 'Sat', 'Sun']
tips['day_ordered'] = pd.Categorical(tips['day'], categories=day_order, ordered=True)
tips_sorted = tips.sort_values('day_ordered')

# Calculate cumulative sum of total_bill over the sorted days
tips_sorted['cumulative_total_bill'] = tips_sorted['total_bill'].cumsum()

fig_line = px.line(tips_sorted,
                   x=tips_sorted.index, # Use index as a sequential x-axis proxy
                   y='cumulative_total_bill',
                   color='day', # Color by day to show progression
                   title='Cumulative Total Bill Over Time (Ordered by Day)',
                   labels={'index': 'Order of Entry (Sorted by Day)', 'cumulative_total_bill': 'Cumulative Total Bill ($)'})

# Display the plot.
fig_line.show()
"
_6y8nyz3V5mp,"import pandas as pd

# 5. Create an interactive line plot using plotly.express.
# For simplicity, we'll sort by day and plot a cumulative sum to show a trend-like visualization.
# First, ensure the 'day' column has a defined order for plotting.
day_order = ['Thur', 'Fri', 'Sat', 'Sun']
tips['day_ordered'] = pd.Categorical(tips['day'], categories=day_order, ordered=True)
tips_sorted = tips.sort_values('day_ordered')

# Calculate cumulative sum of total_bill over the sorted days
tips_sorted['cumulative_total_bill'] = tips_sorted['total_bill'].cumsum()

fig_line = px.line(tips_sorted,
                   x=tips_sorted.index, # Use index as a sequential x-axis proxy
                   y='cumulative_total_bill',
                   color='day', # Color by day to show progression
                   title='Cumulative Total Bill Over Time (Ordered by Day)',
                   labels={'index': 'Order of Entry (Sorted by Day)', 'cumulative_total_bill': 'Cumulative Total Bill ($)'})

# Display the plot.
fig_line.show()
"
8d2a7da5,"print(""--- Comparison of Matplotlib, Seaborn, and Plotly ---"")
print(""\n**1. Level of Abstraction:**"")
print(""- Matplotlib: Low-level. Provides fine-grained control over plot elements but requires more code for complex plots."")
print(""- Seaborn: High-level. Built on Matplotlib, simplifies creating aesthetically pleasing and statistically informative plots."")
print(""- Plotly: High-level. Offers a high-level API (Plotly Express) for quick plotting and a lower-level API (graph_objects) for detailed customization. Focuses on interactive plots."")

print(""\n**2. Focus:**"")
print(""- Matplotlib: General-purpose plotting library for creating a wide variety of static plots."")
print(""- Seaborn: Primarily focused on statistical data visualization, providing functions for exploring relationships and distributions."")
print(""- Plotly: Focused on creating interactive, web-based visualizations that can be embedded in dashboards or exported as HTML."")

print(""\n**3. Ease of Use for Common Tasks:**"")
print(""- Matplotlib: Can be verbose for common tasks like adding labels or legends, especially for complex layouts."")
print(""- Seaborn: Generally easier than Matplotlib for creating standard statistical plots with good default aesthetics."")
print(""- Plotly: Plotly Express is very easy for creating many standard interactive plots. Graph_objects requires more code but offers full control."")

print(""\n**4. Customization Capabilities:**"")
print(""- Matplotlib: Highly customizable at a granular level. Allows control over almost every visual aspect."")
print(""- Seaborn: Provides good customization options, often through parameters in its functions or by accessing the underlying Matplotlib objects."")
print(""- Plotly: Offers extensive customization through its figure dictionaries or graph objects API, allowing control over layout, annotations, hover information, etc."")

print(""\n**5. Output Format:**"")
print(""- Matplotlib: Primarily generates static images (PNG, JPG, PDF, SVG) and can be used in interactive environments."")
print(""- Seaborn: Also primarily generates static images, leveraging Matplotlib's output capabilities."")
print(""- Plotly: Generates interactive plots that can be displayed inline in notebooks, saved as HTML files, or used in web applications."")

print(""\n--- Guidance on When to Use Each Library ---"")
print(""\n**Use Matplotlib when:**"")
print(""- You need complete, fine-grained control over every aspect of your plot."")
print(""- You are creating simple, basic plots where the overhead of other libraries is not necessary."")
print(""- You are building custom plot types or need to integrate tightly with other libraries that expect Matplotlib objects."")
print(""- Your primary output is static images and you don't require interactivity."")

print(""\n**Use Seaborn when:**"")
print(""- You are focused on creating statistical visualizations (e.g., distributions, relationships, categorical comparisons)."")
print(""- You want to quickly generate aesthetically pleasing plots with good default styles."")
print(""- You are working with pandas DataFrames and want easy mapping of data columns to visual attributes."")
print(""- You need common statistical plots like box plots, violin plots, heatmaps, or regression plots with minimal code."")

print(""\n**Use Plotly when:**"")
print(""- You need interactive visualizations (zooming, panning, hovering) for data exploration or presentation."")
print(""- Your target audience or sharing method requires web-based plots (e.g., dashboards, websites, standalone HTML)."")
print(""- You want to create plots that can be easily embedded or shared without requiring users to run Python code."")
print(""- You are working with large datasets where interactivity can help in exploring specific data points."")
"
ecfb329a,"# 1. Print an explanation of what feature engineering means.
print(""--- What is Feature Engineering? ---"")
print(""Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work."")
print(""It involves transforming raw data into a format that is more suitable and informative for the model to learn from."")
print(""Essentially, it's about creating new input features or transforming existing ones to improve the performance of a model."")

# 2. Print a summary explaining why feature engineering is important.
print(""\n--- Why is Feature Engineering Important? ---"")
print(""Feature engineering is crucial because the success of a machine learning model heavily depends on the quality and relevance of the input features."")
print(""Well-engineered features can:"")
print(""- Improve model accuracy and performance."")
print(""- Help algorithms converge faster."")
print(""- Lead to more interpretable models."")
print(""- Reduce the need for complex models by making patterns more explicit."")
print(""- Handle issues like missing values, outliers, and skewed distributions."")

# 3. Print an outline of the typical steps involved in the feature engineering process.
print(""\n--- General Process of Feature Engineering ---"")
print(""The typical steps involved in feature engineering include:"")
print(""1. **Understanding the Data:** Deeply analyze the raw data, its structure, types, and potential issues (missing values, outliers, etc.). Understand the domain and the problem you are trying to solve."")
print(""2. **Brainstorming Features:** Based on your understanding and domain knowledge, brainstorm potential new features that could be relevant to the model."")
print(""3. **Creating Features:** Implement the techniques to create the new features or transform existing ones (e.g., handling missing values, encoding categorical data, scaling numerical data, extracting information from text or dates)."")
print(""4. **Evaluating Features:** Assess the quality and potential impact of the newly created or transformed features. This can involve visualization, statistical tests, or evaluating model performance with and without the features."")
print(""5. **Selecting Features:** Choose the most relevant and impactful features for the model, potentially using feature selection techniques to reduce dimensionality and noise."")
print(""6. **Iterating:** Feature engineering is often an iterative process. You might need to go back to earlier steps based on the evaluation results."")
"
b4a1e22c,"import pandas as pd
import numpy as np

# 1. Print a brief explanation of why handling missing values is important.
print(""--- Handling Missing Data ---"")
print(""\nMissing data is a common issue in real-world datasets and can significantly impact the performance of machine learning models. Many algorithms cannot handle missing values directly, and even those that can might produce biased or inaccurate results."")
print(""Properly handling missing data is crucial to maintain data integrity, avoid errors during model training, and ensure that the model learns from a complete and representative dataset."")

# 2. Discuss common techniques for handling missing values.
print(""\nCommon techniques for handling missing values include:"")
print(""- **Deletion:** Removing rows (listwise deletion) or columns (column-wise deletion) that contain missing values. This is simple but can lead to significant loss of data if many rows or columns have missing values."")
print(""- **Imputation:** Filling in missing values with estimated values. Common imputation strategies include using the mean, median, or mode of the existing data in the column. More advanced methods include using k-Nearest Neighbors (KNN) imputation or model-based imputation."")

# 3. Import the pandas library (already imported in a previous cell)
# import pandas as pd

# 4. Create a sample pandas DataFrame with some missing values (NaN).
data = {'A': [1, 2, np.nan, 4, 5],
        'B': [6, np.nan, 8, 9, 10],
        'C': ['X', 'Y', 'X', np.nan, 'Z'],
        'D': [np.nan, 12, 13, 14, 15]}
df_missing = pd.DataFrame(data)
print(""\n--- Original DataFrame with Missing Values ---"")
display(df_missing)

# 5. Demonstrate deleting rows with missing values using the .dropna() method.
print(""\n--- DataFrame after dropping rows with any missing values ---"")
df_dropped_rows = df_missing.dropna()
display(df_dropped_rows)

# 6. Demonstrate imputing missing values in a numerical column with the mean using the .fillna() method.
print(""\n--- DataFrame after imputing missing values in column 'A' with the mean ---"")
df_imputed_mean = df_missing.copy() # Create a copy to avoid modifying the original DataFrame
mean_A = df_imputed_mean['A'].mean()
df_imputed_mean['A'] = df_imputed_mean['A'].fillna(mean_A)
display(df_imputed_mean)

# 7. Demonstrate imputing missing values in a categorical column with the mode using the .fillna() method.
print(""\n--- DataFrame after imputing missing values in column 'C' with the mode ---"")
df_imputed_mode = df_missing.copy() # Create a copy
# Calculate the mode. .mode() can return multiple values if there's a tie, so take the first one [0].
mode_C = df_imputed_mode['C'].mode()[0]
df_imputed_mode['C'] = df_imputed_mode['C'].fillna(mode_C)
display(df_imputed_mode)
"
3c60041a,"# 1. Print an explanation of why encoding categorical features is necessary.
print(""--- Encoding Categorical Features ---"")
print(""\nCategorical features are variables that contain label values rather than numerical values. Machine learning algorithms typically require numerical input data to perform calculations and learn patterns."")
print(""Therefore, to leverage text data in machine learning models, it must be converted into a numerical format before it can be used to train most models. This process is called categorical encoding."")
print(""Failing to encode categorical data or using an inappropriate encoding method can lead to incorrect model interpretations, biased results, or poor model performance."")

# 2. Discuss common categorical encoding techniques.
print(""\nCommon techniques for encoding categorical features include:"")
print(""- **Label Encoding:** Assigns a unique integer to each category. For example, ['Red', 'Green', 'Blue'] might become [0, 1, 2]."")
print(""  - **When to use:** Suitable for ordinal categorical variables where there is an inherent order or ranking between the categories (e.g., 'Small', 'Medium', 'Large'). However, using it for nominal variables (where there is no order) can introduce an artificial sense of order that might mislead the model."")
print(""- **One-Hot Encoding:** Creates a new binary column for each unique category. If a data point belongs to a category, the corresponding column for that category will have a value of 1, and all other category columns will have 0."")
print(""  - **When to use:** Ideal for nominal categorical variables where there is no intrinsic order between categories (e.g., 'City', 'Color', 'Gender'). It avoids implying any ordinal relationship between categories. However, it can lead to a large number of new columns (high dimensionality) if there are many unique categories."")
"
576c6844,"# 3. Import the LabelEncoder and OneHotEncoder classes from sklearn.preprocessing.
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
# Also need numpy for the OneHotEncoder input format
import numpy as np
import pandas as pd # Import pandas

# 4. Create a sample pandas DataFrame with a categorical column.
data_categorical = {'Color': ['Red', 'Blue', 'Green', 'Red', 'Blue', 'Yellow', 'Green']}
df_categorical = pd.DataFrame(data_categorical)
print(""\n--- Original DataFrame with Categorical Column ---"")
display(df_categorical)

# 5. Demonstrate Label Encoding on the categorical column.
print(""\n--- DataFrame after Label Encoding ('Color') ---"")
label_encoder = LabelEncoder()
df_categorical['Color_LabelEncoded'] = label_encoder.fit_transform(df_categorical['Color'])
display(df_categorical)

# 6. Demonstrate One-Hot Encoding on the categorical column.
print(""\n--- DataFrame after One-Hot Encoding ('Color') ---"")
# OneHotEncoder expects input as a 2D array, so reshape the column
onehot_encoder = OneHotEncoder(sparse_output=False) # Use sparse_output=False for a dense NumPy array
color_onehot = onehot_encoder.fit_transform(df_categorical[['Color']])

# Create a new DataFrame from the one-hot encoded array
# Get the feature names for the new columns
feature_names = onehot_encoder.get_feature_names_out(['Color'])
df_onehot = pd.DataFrame(color_onehot, columns=feature_names, index=df_categorical.index)

# Concatenate the original DataFrame (excluding the original categorical column) and the new one-hot encoded DataFrame
# It's often useful to drop the original column after encoding
df_combined_onehot = pd.concat([df_categorical.drop('Color', axis=1), df_onehot], axis=1)

display(df_combined_onehot)
"
f85d5670,"# 1. Print an explanation of why transforming numerical features is important.
print(""--- Transforming Numerical Features ---"")
print(""\nNumerical features often have varying scales, distributions, and relationships that can negatively impact the performance and training speed of many machine learning algorithms."")
print(""Transforming numerical features helps to:"")
print(""- Standardize or normalize the range of values, preventing features with larger values from dominating the learning process (important for distance-based algorithms like K-Means, SVMs, and algorithms that use gradient descent)."")
print(""- Make the distribution of features more Gaussian-like, which can improve the performance of models that assume normally distributed inputs (e.g., linear regression, logistic regression)."")
print(""- Capture non-linear relationships between features and the target variable (e.g., using polynomial features)."")
print(""- Handle skewed distributions."")

# 2. Discuss common numerical feature transformation techniques.
print(""\nCommon techniques for transforming numerical features include:"")
print(""- **Scaling:** Rescaling features to a specific range (e.g., [0, 1] or [-1, 1]) or to have a mean of 0 and a standard deviation of 1."")
print(""  - **Purpose:** To ensure that all features contribute equally to the model, especially when features have vastly different scales."")
print(""- **Normalization:** Transforming features to have a Gaussian-like distribution. This often involves non-linear transformations like logarithmic, square root, or Box-Cox transformations."")
print(""  - **Purpose:** To satisfy the normality assumptions of certain models and reduce the impact of outliers."")
print(""- **Binning (Discretization):** Converting continuous numerical features into discrete categories or bins."")
print(""  - **Purpose:** To handle outliers, reduce the number of unique values (simplifying the model), or capture non-linear relationships by treating different ranges of values as distinct categories."")
print(""- **Polynomial Features:** Creating new features by raising existing features to a power or combining them through multiplication."")
print(""  - **Purpose:** To introduce non-linearity into the model and capture more complex relationships between features and the target."")
"
0a5f80e2,"# 3. Import the necessary classes from sklearn.preprocessing: MinMaxScaler, StandardScaler, and PolynomialFeatures.
from sklearn.preprocessing import MinMaxScaler, StandardScaler, PolynomialFeatures
# Also, import numpy for creating sample data and pandas for easier handling and binning demonstration.
import numpy as np
import pandas as pd

# 4. Create a sample numpy array or pandas DataFrame with numerical data.
# Include data with different scales and potentially a skewed distribution.
data_numerical = {'Feature1_SmallScale': np.random.rand(20) * 10, # Range ~ 0-10
                  'Feature2_LargeScale': np.random.rand(20) * 1000 + 500, # Range ~ 500-1500
                  'Feature3_Skewed': np.random.chisquare(df=5, size=20) * 50, # Skewed distribution
                  'Feature4_Uniform': np.random.uniform(low=100, high=200, size=20)} # Uniform distribution

df_numerical = pd.DataFrame(data_numerical)

print(""\n--- Original Numerical DataFrame ---"")
display(df_numerical)
"
54d222ee,"# 5. Demonstrate Min-Max Scaling using MinMaxScaler.
print(""\n--- DataFrame after Min-Max Scaling ('Feature2_LargeScale') ---"")
minmax_scaler = MinMaxScaler()
# Apply to a single column for demonstration, reshaping to a 2D array
df_numerical['Feature2_LargeScale_Scaled_MinMax'] = minmax_scaler.fit_transform(df_numerical[['Feature2_LargeScale']])
display(df_numerical[['Feature2_LargeScale', 'Feature2_LargeScale_Scaled_MinMax']].head())

# 6. Demonstrate Standardization (Z-score normalization) using StandardScaler.
print(""\n--- DataFrame after Standardization ('Feature1_SmallScale') ---"")
standard_scaler = StandardScaler()
# Apply to a single column for demonstration, reshaping to a 2D array
df_numerical['Feature1_SmallScale_Scaled_Standard'] = standard_scaler.fit_transform(df_numerical[['Feature1_SmallScale']])
display(df_numerical[['Feature1_SmallScale', 'Feature1_SmallScale_Scaled_Standard']].head())
"
ead41d0d,"# 7. Demonstrate Binning (Discretization) using pd.cut.
print(""\n--- DataFrame after Binning ('Feature3_Skewed') ---"")
# Use pd.cut to divide 'Feature3_Skewed' into 5 bins
# labels=False will return integer labels for bins 0 to n-1
df_numerical['Feature3_Skewed_Binned'] = pd.cut(df_numerical['Feature3_Skewed'], bins=5, labels=False, include_lowest=True)
display(df_numerical[['Feature3_Skewed', 'Feature3_Skewed_Binned']].head())

# 8. Demonstrate creating Polynomial Features using PolynomialFeatures.
print(""\n--- Original Features and Polynomial Features (Degree 2) ---"")
# Select a couple of features to create polynomial features from
features_for_poly = df_numerical[['Feature1_SmallScale', 'Feature4_Uniform']]

# Create polynomial features up to degree 2 (includes interactions and original features)
poly_transformer = PolynomialFeatures(degree=2, include_bias=False)
poly_features = poly_transformer.fit_transform(features_for_poly)

# Create a DataFrame for the polynomial features
# Get the names of the new features
poly_feature_names = poly_transformer.get_feature_names_out(['Feature1_SmallScale', 'Feature4_Uniform'])
df_polynomial_features = pd.DataFrame(poly_features, columns=poly_feature_names, index=df_numerical.index)

# Display the original features and the new polynomial features
display(pd.concat([features_for_poly, df_polynomial_features], axis=1).head())
"
1764a70f,"# 1. Print an explanation of why converting text data into numerical features is necessary.
print(""--- Converting Text Data to Numerical Features ---"")
print(""\nText data, while rich in information, is inherently unstructured and non-numerical. Most machine learning algorithms are designed to work with numerical inputs."")
print(""Therefore, to leverage text data in machine learning models, it must be converted into a numerical representation. This process allows algorithms to quantify and process linguistic information."")

# 2. Discuss common techniques for transforming text data into numerical formats.
print(""\nCommon techniques for transforming text data into numerical formats include:"")
print(""- **Bag-of-Words (BoW):** This is a simple and commonly used model. It represents a text document as the multiset (bag) of its words, disregarding grammar and even word order, but keeping multiplicity."")
print(""  - **How it works:** It creates a vocabulary of all unique words in the entire corpus of documents. Each document is then represented as a vector where each element corresponds to a word in the vocabulary, and the value is the frequency (count) of that word in the document."")
print(""  - **Strength:** Simple to understand and implement."")
print(""  - **Weakness:** Loses word order information, can result in high-dimensional sparse vectors (especially with large vocabularies), and treats all words equally regardless of their importance."")

print(""- **TF-IDF (Term Frequency-Inverse Document Frequency):** This technique reflects how important a word is to a document relative to the entire corpus."")
print(""  - **How it works:** It calculates a score for each word in each document based on two components:"")
print(""    - **Term Frequency (TF):** How often a word appears in a document."")
print(""    - **Inverse Document Frequency (IDF):** A measure of how rare a word is across all documents. Words that appear in many documents have a lower IDF score (and thus a lower overall TF-IDF score)."")
print(""  - **Strength:** Considers the importance of words by downweighting common words (like 'the', 'a', 'is') and highlighting words that are more specific to a document."")
print(""  - **Weakness:** Still loses word order information and can also result in high-dimensional sparse vectors."")
"
085a9d8d,"# 3. Import the CountVectorizer and TfidfVectorizer classes from sklearn.feature_extraction.text.
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
import pandas as pd # Import pandas to create a Series

# 4. Create a sample list or pandas Series of text documents.
sample_documents = [
    ""This is the first document."",
    ""This document is the second document."",
    ""And this is the third one."",
    ""Is this the first document?"",
]
sample_series = pd.Series(sample_documents)

print(""\n--- Sample Text Documents ---"")
display(sample_series)

# 5. Demonstrate the Bag-of-Words approach using CountVectorizer.
print(""\n--- Bag-of-Words Representation using CountVectorizer ---"")
count_vectorizer = CountVectorizer()
# Fit the vectorizer to the documents and transform them into a matrix of token counts
bag_of_words_matrix = count_vectorizer.fit_transform(sample_series)

# Display the resulting sparse matrix
print(""\nSparse Matrix (CountVectorizer):"")
print(bag_of_words_matrix)

# Display the vocabulary learned by the vectorizer
print(""\nVocabulary (CountVectorizer):"")
print(count_vectorizer.vocabulary_)

# Optionally, display the dense array representation
print(""\nDense Array (CountVectorizer):"")
print(bag_of_words_matrix.toarray())
"
5c445d12,"# 6. Demonstrate the TF-IDF approach using TfidfVectorizer.
print(""\n--- TF-IDF Representation using TfidfVectorizer ---"")
tfidf_vectorizer = TfidfVectorizer()
# Fit the vectorizer to the documents and transform them into a TF-IDF matrix
tfidf_matrix = tfidf_vectorizer.fit_transform(sample_series)

# Display the resulting sparse matrix
print(""\nSparse Matrix (TfidfVectorizer):"")
print(tfidf_matrix)

# Display the vocabulary learned by the vectorizer
print(""\nVocabulary (TfidfVectorizer):"")
print(tfidf_vectorizer.vocabulary_)

# Optionally, display the dense array representation
print(""\nDense Array (TfidfVectorizer):"")
print(tfidf_matrix.toarray())
"
819bc40e,"# 1. Print an explanation of why extracting features from date and time data is valuable.
print(""--- Extracting Features from Date and Time Data ---"")
print(""\nDate and time information in a dataset often contains valuable patterns and signals that standard machine learning models cannot directly interpret in their raw format (e.g., 'YYYY-MM-DD HH:MM:SS')."")
print(""Extracting relevant features from datetime data can help models understand temporal trends, seasonality, cyclic patterns, and specific event occurrences (like holidays or weekends)."")
print(""These extracted features can significantly improve the model's ability to capture time-dependent relationships and make more accurate predictions, especially in time series forecasting, anomaly detection, and behavioral analysis."")

# 2. Discuss common features that can be extracted from date and time information.
print(""\nCommon features that can be extracted from date and time information include:"")
print(""- **Year:** The year of the observation."")
print(""- **Month:** The month of the year (1-12)."")
print(""- **Day:** The day of the month (1-31)."")
print(""- **Day of the week:** The day of the week (e.g., Monday, Tuesday, or 0-6)."")
print(""- **Day of the year:** The day of the year (1-365 or 366)."")
print(""- **Week of the year:** The week number of the year."")
print(""- **Hour:** The hour of the day (0-23)."")
print(""- **Minute:** The minute of the hour (0-59)."")
print(""- **Second:** The second of the minute (0-59)."")
print(""- **Quarter:** The quarter of the year."")
print(""- **Weekend/Weekday flag:** A boolean or binary indicator if the date falls on a weekend."")
print(""- **Season:** Mapping months or dates to seasons (e.g., Spring, Summer, Autumn, Winter)."")
print(""- **Time of day:** Categorizing hours into broader periods (e.g., Morning, Afternoon, Evening, Night)."")
print(""- **Lag features:** Values from previous time steps."")
print(""- **Rolling window features:** Aggregated statistics (mean, sum, etc.) over a recent time window."")
"
dbd8bc92,"# 4. Create a sample pandas DataFrame with a datetime column.
data_datetime = {'Timestamp': pd.to_datetime(['2023-01-15 10:30:00',
                                              '2023-01-16 14:45:00',
                                              '2023-02-20 09:00:00',
                                              '2023-03-10 18:15:00',
                                              '2023-04-01 11:00:00',
                                              '2023-04-02 22:00:00',
                                              '2024-07-04 05:30:00', # Leap year example
                                              '2024-12-25 12:00:00'])}

df_datetime = pd.DataFrame(data_datetime)

print(""\n--- Original DataFrame with Datetime Column ---"")
display(df_datetime)

# 5. Demonstrate extracting various features from the datetime column.
print(""\n--- DataFrame with Extracted Datetime Features ---"")
df_datetime['Year'] = df_datetime['Timestamp'].dt.year
df_datetime['Month'] = df_datetime['Timestamp'].dt.month
df_datetime['Day'] = df_datetime['Timestamp'].dt.day
df_datetime['DayOfWeek'] = df_datetime['Timestamp'].dt.dayofweek # Monday=0, Sunday=6
df_datetime['DayName'] = df_datetime['Timestamp'].dt.day_name()
df_datetime['Hour'] = df_datetime['Timestamp'].dt.hour
df_datetime['Minute'] = df_datetime['Timestamp'].dt.minute

# 6. Demonstrate creating a boolean feature for whether a date falls on a weekend.
# Weekend is Saturday (5) or Sunday (6)
df_datetime['IsWeekend'] = df_datetime['DayOfWeek'].apply(lambda x: x >= 5)

# 7. Display the DataFrame with the newly extracted date and time features.
display(df_datetime)
"
f1cfd8fe,"# 1. Print an explanation of why creating new features is beneficial.
print(""--- Creating New Features from Existing Ones ---"")
print(""\nCreating new features from existing ones is a fundamental aspect of feature engineering that can significantly boost model performance."")
print(""Raw features might not always directly reveal the most relevant patterns or relationships for a machine learning algorithm."")
print(""By combining or transforming existing features, we can often create new features that are more informative, capture complex interactions, or encode domain-specific knowledge that was not explicit in the original data."")
print(""This process can help the model learn more effectively, leading to improved accuracy, better generalization, and sometimes, more interpretable results."")

# 2. Discuss common techniques for creating new features.
print(""\nCommon techniques for creating new features from existing ones include:"")
print(""- **Aggregation:** Summarizing information across multiple related data points or over time. Examples include calculating the total sales per customer, the average rating for a product, or the sum of events in a time window."")
print(""  - **Purpose:** To condense information and reveal patterns at a higher level of granularity."")
print(""- **Interactions:** Combining two or more features to capture their synergistic effect. This is often done through multiplication (e.g., `Feature1 * Feature2`) or other mathematical operations."")
print(""  - **Purpose:** To model relationships where the effect of one feature depends on the value of another."")
print(""- **Using Domain Knowledge:** Leveraging expertise about the problem domain to create relevant features. This could involve creating ratios, differences, flags for specific conditions, or combining features in ways that reflect real-world processes."")
print(""  - **Purpose:** To encode insights that are not evident from the raw data alone and guide the model towards meaningful patterns."")
print(""- **Polynomial Features:** Creating new features by raising existing numerical features to powers (e.g., `x^2`, `x^3`) or creating interaction terms (e.g., `x*y`). (Already discussed in numerical feature handling, but relevant here as a method of creating new features)."""
6d6d59b4,"import pandas as pd
import numpy as np

# 3. Create a sample pandas DataFrame with numerical columns.
data_new_features = {
    'Feature_A': np.random.rand(10) * 100,
    'Feature_B': np.random.rand(10) * 50,
    'Feature_C': np.random.randint(1, 20, size=10),
    'Feature_D': np.random.uniform(5, 25, size=10)
}
df_new_features = pd.DataFrame(data_new_features)

print(""--- Original DataFrame ---"")
display(df_new_features)

# 4. Demonstrate creating a new feature by combining existing features through a mathematical operation.
# Example: Create a feature which is the sum of Feature_A and Feature_B
df_new_features['Feature_A_plus_B'] = df_new_features['Feature_A'] + df_new_features['Feature_B']

# Example: Create a feature which is the ratio of Feature_A to Feature_B (handle potential division by zero)
# Add a small epsilon to the denominator to avoid division by zero if Feature_B can be zero
epsilon = 1e-6
df_new_features['Feature_A_div_B'] = df_new_features['Feature_A'] / (df_new_features['Feature_B'] + epsilon)

# 5. Demonstrate creating an interaction feature by multiplying two existing features.
# Example: Create an interaction feature between Feature_A and Feature_C
df_new_features['Feature_A_times_C'] = df_new_features['Feature_A'] * df_new_features['Feature_C']

# 6. Demonstrate creating a new feature based on conditional logic or binning.
# Example: Create a binary feature indicating if Feature_A is above a certain threshold (e.g., the median)
threshold_A = df_new_features['Feature_A'].median()
df_new_features['Feature_A_Above_Median'] = (df_new_features['Feature_A'] > threshold_A).astype(int)

# Example: Create a categorical feature by binning Feature_D
df_new_features['Feature_D_Binned'] = pd.cut(df_new_features['Feature_D'], bins=3, labels=['Low', 'Medium', 'High'])

# 7. Display the DataFrame with the newly created features.
print(""\n--- DataFrame with Newly Created Features ---"")
display(df_new_features)
"
52800aa2,"# 1. Print an explanation of why feature selection is important.
print(""--- Feature Selection ---"")
print(""\nFeature selection is the process of choosing a subset of the most relevant features (variables, predictors) to be used in a machine learning model."")
print(""Using irrelevant, redundant, or noisy features can negatively impact model performance by:"")
print(""- Increasing training time and computational cost."")
print(""- Reducing model accuracy (due to overfitting or capturing noise)."")
print(""- Making the model harder to interpret."")
print(""- Increasing the complexity of data collection and processing."")
print(""Feature selection helps to mitigate these issues by simplifying models, improving performance, and reducing computational overhead."")

# 2. Discuss common methods for selecting the most relevant features.
print(""\nCommon methods for feature selection can be broadly categorized into:"")
print(""- **Filter Methods:** These methods select features based on their statistical properties or their relationship with the target variable, independently of the machine learning algorithm."")
print(""  - Examples: Correlation coefficient, Chi-squared test, ANOVA F-value, Mutual Information."")
print(""  - Strength: Fast and computationally inexpensive."")
print(""  - Weakness: Do not consider interactions between features and might select redundant features."")
print(""\n- **Wrapper Methods:** These methods use a specific machine learning algorithm to evaluate the performance of different subsets of features. They iteratively select or remove features based on how the model's performance changes."")
print(""  - Examples: Recursive Feature Elimination (RFE), Sequential Feature Selection."")
print(""  - Strength: Can capture feature interactions and are tailored to the specific model."")
print(""  - Weakness: Computationally more expensive than filter methods, prone to overfitting to the evaluation metric."")
print(""\n- **Embedded Methods:** These methods perform feature selection as part of the model training process itself. Some algorithms have built-in feature selection capabilities."")
print(""  - Examples: Lasso regularization (L1), Ridge regularization (L2), Decision Trees, Random Forests."")
print(""  - Strength: Often less computationally expensive than wrapper methods, interact with the model."")
print(""  - Weakness: The selected features are often specific to the chosen model."")
"
8a5dd0ea,"# 3. Import necessary libraries for a simple example
from sklearn.feature_selection import SelectKBest, f_classif
import pandas as pd
import numpy as np

# 4. Create a sample pandas DataFrame with features and a target variable.
# Include some features that are less relevant to the target.
np.random.seed(42) # for reproducibility

# Relevant features
X_relevant = np.random.rand(100, 2) * 10 # Two relevant features
y = (X_relevant[:, 0] + X_relevant[:, 1] + np.random.randn(100) * 2 > 10).astype(int) # Target variable based on sum

# Irrelevant features (random noise)
X_irrelevant = np.random.randn(100, 3) * 5 # Three irrelevant features

# Combine relevant and irrelevant features
X = np.hstack((X_relevant, X_irrelevant))

# Create DataFrame
feature_names = [f'Feature_{i+1}' for i in range(X.shape[1])]
df_features = pd.DataFrame(X, columns=feature_names)
df_features['Target'] = y

print(""\n--- Sample DataFrame with Relevant and Irrelevant Features ---"")
display(df_features.head())

# 5. Demonstrate using a filter method like SelectKBest with f_classif.
# Select the top K features (let's say K=2, aiming to pick the two relevant ones)
k = 2
selector = SelectKBest(score_func=f_classif, k=k)

# Fit the selector to the features and target
selector.fit(df_features.drop('Target', axis=1), df_features['Target'])

# Get the indices of the selected features
selected_feature_indices = selector.get_support(indices=True)

# Get the names of the selected features
selected_feature_names = [feature_names[i] for i in selected_feature_indices]

# 6. Print the selected feature indices or names.
print(f""\n--- Selected Top {k} Features (using SelectKBest with f_classif) ---"")
print(""Selected feature indices:"", selected_feature_indices)
print(""Selected feature names:"", selected_feature_names)

# Optionally, display the scores for all features
print(""\nFeature scores:"")
scores = selector.scores_
for i, score in enumerate(scores):
    print(f""{feature_names[i]}: {score:.2f}"")
"
f623500f,"from sklearn.preprocessing import LabelEncoder, MinMaxScaler, PolynomialFeatures
from sklearn.feature_selection import SelectKBest, f_classif
import pandas as pd
import numpy as np
"
1b55db3a,"# 2. Create a sample pandas DataFrame with a mix of data types (numerical, categorical, datetime) and potential issues (missing values).
data = {
    'Numerical_Feature_1': [10.5, 12.1, np.nan, 15.8, 11.9, 13.5, 14.2, 10.1, 16.5, 12.8],
    'Numerical_Feature_2': [100, 120, 110, 150, 115, np.nan, 145, 105, 160, 130],
    'Categorical_Feature': ['A', 'B', 'A', 'C', 'B', 'C', 'A', 'B', 'C', 'A'],
    'Categorical_With_Missing': ['X', 'Y', 'X', np.nan, 'Y', 'Z', 'X', 'Y', 'Z', 'X'],
    'Datetime_Feature': pd.to_datetime(['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15', '2023-03-01', '2023-03-15', '2023-04-01', '2023-04-15', '2023-05-01', '2023-05-15']),
    'Target': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1] # Sample target variable for feature selection later
}
df = pd.DataFrame(data)

print(""--- Original Sample DataFrame ---"")
display(df)
"
b2c89cbd,"# 3. Handle missing values using a suitable imputation strategy.
# Impute numerical features with the mean
for col in ['Numerical_Feature_1', 'Numerical_Feature_2']:
    if df[col].isnull().any():
        mean_val = df[col].mean()
        df[col] = df[col].fillna(mean_val)

# Impute categorical feature with the mode
if df['Categorical_With_Missing'].isnull().any():
    mode_val = df['Categorical_With_Missing'].mode()[0]
    df['Categorical_With_Missing'] = df['Categorical_With_Missing'].fillna(mode_val)

print(""\n--- DataFrame after Imputing Missing Values ---"")
display(df)

# 4. Encode categorical features using an appropriate method (e.g., One-Hot Encoding).
# Use One-Hot Encoding for 'Categorical_Feature' and 'Categorical_With_Missing'
df = pd.get_dummies(df, columns=['Categorical_Feature', 'Categorical_With_Missing'], drop_first=True)

print(""\n--- DataFrame after One-Hot Encoding ---"")
display(df)

# 5. Extract features from the datetime column (e.g., year, month, day of week).
df['Year'] = df['Datetime_Feature'].dt.year
df['Month'] = df['Datetime_Feature'].dt.month
df['Day'] = df['Datetime_Feature'].dt.day
df['DayOfWeek'] = df['Datetime_Feature'].dt.dayofweek
df = df.drop('Datetime_Feature', axis=1) # Drop the original datetime column

print(""\n--- DataFrame after Extracting Datetime Features ---"")
display(df)

# 6. Apply a numerical transformation (e.g., Min-Max Scaling) to a numerical feature.
scaler = MinMaxScaler()
df['Numerical_Feature_1_Scaled'] = scaler.fit_transform(df[['Numerical_Feature_1']])

print(""\n--- DataFrame after Min-Max Scaling ---"")
display(df)

# 7. Create a new feature by combining existing features (e.g., sum or ratio).
df['Numerical_Features_Sum'] = df['Numerical_Feature_1'] + df['Numerical_Feature_2']

print(""\n--- DataFrame after Creating New Feature ---"")
display(df)

# 8. Briefly demonstrate a simple feature selection step (e.g., using SelectKBest).
# Assuming 'Target' is the target variable
X = df.drop('Target', axis=1)
y = df['Target']

# Select the top K features (e.g., top 5)
k = 5
selector = SelectKBest(score_func=f_classif, k=k)

# Fit the selector
selector.fit(X, y)

# Get the names of the selected features
selected_features_indices = selector.get_support(indices=True)
selected_feature_names = X.columns[selected_features_indices].tolist()

print(f""\n--- Top {k} Selected Features (using SelectKBest) ---"")
print(selected_feature_names)

# 9. Display the resulting DataFrame after applying these techniques. (Already displayed after each major step, but displaying the final one again)
print(""\n--- Final DataFrame after Feature Engineering Steps ---"")
display(df)
"
